{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仲景基礎對話功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\"  # replace with your device: \"cpu\", \"cuda\", \"mps\"\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "peft_model_id = \"CMLM/ZhongJing-2-1_8b\"\n",
    "base_model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "model.load_adapter(peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"CMLM/ZhongJing-2-1_8b\", padding_side=\"right\", trust_remote_code=True, pad_token=\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_model_response(question):\n",
    "    # Create the prompt without context\n",
    "    prompt = f\"Question: {question}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # Prepare the input\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "\n",
    "# Define a Gradio interface without the context parameter\n",
    "def chat_interface(question):\n",
    "    response = get_model_response(question)\n",
    "    return response\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_interface,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"仲景GPT-V2-1.8B\",\n",
    "    description=\"博极医源，精勤不倦。Unlocking the Wisdom of Traditional Chinese Medicine with AI.\",\n",
    ")\n",
    "\n",
    "# Launch the interface with sharing enabled\n",
    "# iface.launch(share=True)\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
